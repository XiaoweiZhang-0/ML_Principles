{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hw2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNelJoUdQyvrplCY/bLuER5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"7Q2ghgsdUbTZ"},"source":["First, import some useful packages and set everything up."]},{"cell_type":"code","metadata":{"id":"3YC7lMVUzF75"},"source":["import copy\n","import csv\n","import matplotlib \n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import pandas\n","import random\n","import seaborn\n","\n","from collections import Counter\n","\n","def set_seed(seed):  # For reproducibility, fix random seeds.\n","  random.seed(seed)\n","  np.random.seed(seed)\n","\n","set_seed(42)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uRrB7306Ux0J"},"source":["# Data"]},{"cell_type":"markdown","metadata":{"id":"tAhJbEBvUyfw"},"source":["Download the MNIST data from the [course Kaggle page](https://www.kaggle.com/c/rutgers-cs461-hw2-fall-2021). We will assume that we have the directory `data/mnist/` in our Google Drive account. Let's load the data and stare at it.  "]},{"cell_type":"code","metadata":{"id":"U8s247fdVWVW"},"source":["# Load the Drive helper and mount. You will have to authorize this operation. \n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eGiG_9SiVaZd"},"source":["datadir = '/content/drive/My Drive/data/MNIST/'\n","\n","class MNISTDataset:\n","\n","  def __init__(self, split):\n","    assert split in ['train', 'train_small', 'val', 'test']\n","    self.inputs = np.load('{:s}inputs_{:s}.npy'.format(datadir, split))\n","    self.labels = None\n","    if split != 'test':\n","      self.labels, self.label_count = self.get_labels('{:s}labels_{:s}.npy'.format(datadir, split))\n","      assert self.labels.shape[0] == self.inputs.shape[0]\n","    self.split = split\n","\n","  def get_labels(self, filepath):\n","    label_matrix = np.load(filepath)  # (num_examples, num_labels)\n","    labels = np.zeros((len(label_matrix), 1)).astype(int)  # (num_examples, 1)\n","    label_count = Counter()\n","    for i in range(len(label_matrix)):\n","      label= np.nonzero(label_matrix[i])[0][0]\n","      labels[i] = label\n","      label_count[label] += 1\n","\n","    return labels, label_count\n","\n","  def num_examples(self):\n","    return self.inputs.shape[0]\n","\n","  def dim(self):\n","    return self.inputs.shape[1]\n","\n","  def generate_batch(self, batch_size):\n","    inds = list(range(self.num_examples()))  \n","    if self.split == 'train':  # If train, shuffle example indices before generating\n","      random.shuffle(inds)    \n","    for i in range(0, len(inds), batch_size):\n","        inds_batch = inds[i: i + batch_size]\n","        X = self.inputs[inds_batch, :]\n","        y = self.labels[inds_batch, :] if self.labels is not None else None\n","        yield X, y\n","\n","dataset_train = MNISTDataset('train')\n","dataset_val = MNISTDataset('val')\n","dataset_test = MNISTDataset('test')\n","\n","print('Number of examples (train/val/test): {:d}/{:d}/{:d}'.format(dataset_train.num_examples(), dataset_val.num_examples(), dataset_test.num_examples()))\n","print('Original number of features (image represented as a vector): {:d}'.format(dataset_train.dim()))\n","print('First training input looks like this...', dataset_train.inputs[0, 160:200])\n","print('Label: ', dataset_train.labels[0, 0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jqxbDzIBcJpW"},"source":["Note that we don't have labels for the test portion. We can do a better job of visualizing the inputs."]},{"cell_type":"code","metadata":{"id":"mIELtIm2asPM"},"source":["def visualize_image(image_raw):\n","  image = image_raw.reshape(28, 28)  # Reshape a vector into a square\n","  fig = matplotlib.pyplot.figure()\n","  ax = fig.add_subplot(1, 1, 1)\n","  imgplot = ax.imshow(image, cmap=matplotlib.cm.Greys)\n","  imgplot.set_interpolation(\"nearest\")\n","  ax.xaxis.set_ticks_position(\"top\")\n","  ax.yaxis.set_ticks_position(\"left\")\n","  matplotlib.pyplot.axis(\"off\")\n","  matplotlib.pyplot.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QVHl9XNbYRM4"},"source":["print('First training input')\n","visualize_image(dataset_train.inputs[0])\n","print('Label: ', dataset_train.labels[0, 0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DdA4NSCycxov"},"source":["The dataset is balanced so that we have an equal amount of supervision for each class. This is a luxury: in the wild we will need to deal with datasets with possibly very unbalanced datasets."]},{"cell_type":"code","metadata":{"id":"9FpKSbzma2lI"},"source":["for digit in range(10):\n","  print('Digit {:1d} has {:3d} training examples'.format(digit, dataset_train.label_count[digit]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KRItIDZh6I-3"},"source":["## Feature Normalization"]},{"cell_type":"markdown","metadata":{"id":"Dh4ZQKYD8DrW"},"source":["Raw pixel values have high variance and are not mean-centered. To make learning more effective, we'll preprocess the data and normalize each feature (i.e., each pixel) so that each has mean 0 and variance 1 (\"$z$-scoring\"), similar to what we did in the regression assignment."]},{"cell_type":"code","metadata":{"id":"RruuAhR56MRO"},"source":["def normalize_features(X, mu=None, sigma=None):\n","  if mu is None or sigma is None: \n","    mu = X.mean(0)\n","    sigma = X.std(0)\n","    sigma[sigma < 0.0001] = 1  # Avoid division by zero in case of degenerate features.\n","\n","  # Normalize features and also add a bias feature.\n","  X_new = np.concatenate([np.ones((X.shape[0], 1)), (X - mu) / sigma], 1)\n","\n","  return X_new, mu, sigma"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AyNFhV0c8sly"},"source":["print('First training input after normalization (not including the bias dimension)')\n","visualize_image(normalize_features(dataset_train.inputs)[0][0, 1:])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Em7UNZYW_xmi"},"source":["# Softmax "]},{"cell_type":"markdown","metadata":{"id":"nNJ-H2Eg1tqC"},"source":["Define a row-wise softmax that turns any rows of label scores (aka. \"logits\") into probability distributions over labels, with a numerical stability trick."]},{"cell_type":"code","metadata":{"id":"j1svacKC2QJ4"},"source":["def softmax(scores):  # (num_examples, num_labels)\n","  nonnegs = np.exp(scores - np.amax(scores, axis=1)[:, np.newaxis])  # Mitigate numerical overflow by subtracting max \n","  return nonnegs / np.sum(nonnegs, axis=1)[:, np.newaxis]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-rdOEduc_4RM"},"source":["u = np.array([[-1, 2, 0]])\n","print(u[0], '=>', softmax(u)[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YcwTGEd0BPrF"},"source":["Also, define a row-wise logsumexp that computes the log of the sum of the elements of each row, with a numerical stability trick."]},{"cell_type":"code","metadata":{"id":"UaBGpeYPBWs4"},"source":["def logsumexp(scores):  # (num_examples, num_labels)\n","  rowwise_max = np.amax(scores, axis=1)[:, np.newaxis] \n","  return rowwise_max + np.log(np.sum(np.exp(scores - rowwise_max), axis=1)[:, np.newaxis])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l2Mu0EBnWcek"},"source":["v = np.array([[-1, -2, 3]])\n","print(logsumexp(v)[0, 0])\n","print(np.log(np.exp(v).sum()))\n","\n","print()\n","w = np.array([[999, 997, 990]])\n","print(logsumexp(w)[0, 0])\n","print(np.log(np.exp(w).sum()))  # Without the numerical stability trick, we will encounter an overflow here. "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6dJw-Klj1mSL"},"source":["# Model"]},{"cell_type":"markdown","metadata":{"id":"WD9mdCwh45KX"},"source":["Our model is a simple linear classifier that treats each image as a single vector. The feature normalization above introduces a bias dimension, so the model does have a bias parameter. Thus the model parameter is a single matrix $W \\in \\mathbb{R}^{d \\times L}$ where $d$ is the dimension of the input (including the bias dimension) and $L$ is the number of labels (here, 10). The $l$-th column $w_l$ of $W$ computes the score of the given input $x \\in \\mathbb{R}^d$ for the $l$-th label as $w_l^\\top x \\in \\mathbb{R}$. The model has two key functions:\n","\n","- `forward`: Given $N$ images in a minibatch, it computes scores/logits for all $L=10$ labels, which you can represent as an $N \\times L$ matrix. This is all you need to make inference (you just pick argmax), but if it's additionally given gold labels it will also compute the *sum* of per-example negative log probabilities, plus a regularization term, \n","$$\n","- \\sum_{i=1}^N \\log \\frac{\\exp(w_{y_i}^\\top x_i)}{\\sum_{l=1}^L \\exp(w_l^\\top x_i)} + \\lambda \\sum_{l=1}^L \\sum_{j=2}^d W_{j,l}^2\n","$$\n","($\\lambda \\geq 0$ is a hyperparameter that decides the strength of regularization, note that we don't regularize the bias parameters). This is used for monitoring purposes later. More importantly, it will call the next function to calculate gradients.\n","\n","- `accumulate_gradients`: Given $N$ images and gold labels, it computes the gradient of the *average* of per-example negative log probabilities, plus the regularization term, \n","$$ \n","\\hat{J}(W) = - \\frac{1}{N} \\sum_{i=1}^N \\log \\frac{\\exp(w_{y_i}^\\top x_i)}{\\sum_{l=1}^L \\exp(w_l^\\top x_i)} + \\lambda \\sum_{l=1}^L \\sum_{j=2}^d W_{j,l}^2\n","$$\n","with respect to $W$ evaluated at the *current* value of $W$, then accumulates $\\nabla \\hat{J}(W) \\in \\mathbb{R}^{d \\times L}$ for gradient-based optimization."]},{"cell_type":"code","metadata":{"id":"ZwvWJnKS5CbD"},"source":["class LinearClassifier:\n","    \n","  def __init__(self, inputs_train, num_labels, init_range=0.0):\n","    new_features, self.mu, self.sigma = normalize_features(inputs_train)  # Get means and standard devations\n","    self.dim = new_features.shape[1]\n","\n","    # Initialize parameters. \n","    self.W = np.random.uniform(-init_range, init_range, (self.dim, num_labels))\n","\n","    # Initialize the gradient.\n","    self.W_grad = np.zeros((self.dim, num_labels))\n","                      \n","  def forward(self, X_raw, y=None, regularization_weight=0.):\n","    X = normalize_features(X_raw, self.mu, self.sigma)[0]\n","    scores = np.matmul(X, self.W)  # (batch_size, num_labels)        \n","    loss_sum = None\n","    if y is not None:  # We're given gold labels, we're training.\n","\n","      # TODO: Compute the negative log probabilities here using logsumexp. The NumPy function take_along_axis will also be useful.\n","      negative_log_probs = None  # (batch_size,)\n","\n","      squared_norm_W = np.linalg.norm(self.W[1:, :], 'fro') ** 2  # Don't regularize bias parameters\n","      loss_sum = np.sum(negative_log_probs) + regularization_weight * squared_norm_W\n","      self.accumulate_gradients(X, y, scores, regularization_weight)\n","\n","    return loss_sum, scores\n","  \n","  def accumulate_gradients(self, X, y, scores, regularization_weight):\n","    batch_size, num_labels = scores.shape\n","    probs = softmax(scores)        \n","\n","    # TODO: Compute the gradient of the average negative log probability wrt W\n","    loss_grad = None  # (dim, num_labels)\n","\n","    # TODO: Compute the gradient of the regularization term (again, remember that bias parameters are not regularized).\n","    squared_norm_grad = None  # (dim, num_labels)\n","    \n","    self.W_grad += loss_grad + regularization_weight * squared_norm_grad\n","      \n","  def predict(self, X_raw):\n","      _, scores = self.forward(X_raw)\n","      preds = np.argmax(scores, axis=1)[:, np.newaxis]  # (batch_size, 1)\n","      return preds\n","\n","  def zero_grad(self):\n","    self.W_grad.fill(0.)\n","      \n","  def num_parameters(self):\n","    return self.W.size "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vZooqIrkpH7p"},"source":["## Gradient Check"]},{"cell_type":"markdown","metadata":{"id":"ZQ0fvmkkpLW4"},"source":["One useful way to ensure that your gradient computation is absolutely correct is to do what's called the **gradient check**. Recall that *by definition* the derivative of a function $f$ of some scalar variable $x \\in \\mathbb{R}$ evaluated at $x = a$ is:\n","\n","$$\n","f'(a) = \\lim_{\\epsilon \\rightarrow 0} \\frac{f(x + \\epsilon) - f(x)}{\\epsilon}\n","$$\n","\n","Thus we can set some small $\\epsilon > 0$ and check if this is indeed the case for our gradients. Remember, in our case the function of interest is $\\hat{J}(W)$ above, defined on a single batch of labeled examples. We are calculating the gradient of that function with respect to model parameters $W$. Even though this is matrix-valued, a gradient is simply an array of partial derivatives so we can check each derivative individually. "]},{"cell_type":"code","metadata":{"id":"4759rObUphK9"},"source":["import unittest\n","\n","class TestGradient(unittest.TestCase):\n","\n","  def setUp(self):\n","    set_seed(42)\n","    dim = 100\n","    num_labels = 7\n","    num_examples = 10\n","    self.model = LinearClassifier(np.random.randn(num_examples, dim), num_labels, init_range=0.01)\n","    self.X = np.random.randn(num_examples, dim)\n","    self.y = np.random.randint(num_labels, size=(num_examples, 1))\n","    self.epsilon = 1e-4\n","    self.regularization_weight = 0.01\n","    self.loss_avg = self.model.forward(self.X, self.y, self.regularization_weight)[0] / num_examples\n","    self.model.zero_grad()\n","      \n","  def test_gradient_W(self):\n","    for i in range(self.model.W.shape[0]):\n","      for j in range(self.model.W.shape[1]): \n","        self.model.W[i, j] += self.epsilon\n","        loss_avg_perturbed = self.model.forward(self.X, self.y, self.regularization_weight)[0] / self.X.shape[0]\n","        partial_derivative_i_j = self.model.W_grad[i, j]\n","        truth = (loss_avg_perturbed - self.loss_avg) / self.epsilon\n","        error = abs(partial_derivative_i_j - truth)\n","        self.assertLess(error, 1e-3)\n","        self.model.W[i, j] -= self.epsilon\n","        self.model.zero_grad()\n","            \n","unittest.main(TestGradient(), argv=[''], verbosity=2, exit=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XjgjHPUeFNFl"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"id":"Tlwhc9k2FOYF"},"source":["We will use stochastic gradient descent (SGD) to optimize the average cross-entropy loss above. Define a simple SGD optimizer class, which updates parameters of the associated model."]},{"cell_type":"code","metadata":{"id":"fLnk8xMzFVY5"},"source":["class SGDOptimizer:\n","    \n","  def __init__(self, model, learning_rate):\n","    self.model = model\n","    self.lr = learning_rate\n","      \n","  def step(self):\n","    self.model.W -= self.lr * self.model.W_grad\n","      \n","  def zero_grad(self):\n","    self.model.zero_grad()\n","      \n","  def modify_lr(self, learning_rate):\n","    self.lr = learning_rate    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JY9l8cvDFdQi"},"source":["def evaluate_accuracy(model, dataset_eval, batch_size_eval=64):\n","  num_correct = 0\n","  for X, y in dataset_eval.generate_batch(batch_size_eval):\n","    num_correct += np.sum(model.predict(X) == y)\n","  return num_correct / dataset_eval.num_examples() * 100."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jue-LX-mFx9W"},"source":["def train(dataset_train, dataset_val, learning_rate=0.1, init_range=0., batch_size=16, regularization_weight=0., max_num_epochs=10, seed=42, loss_improvement=0.01, decay=2., tolerance=5, verbose=False):\n","  set_seed(seed)  \n","  model = LinearClassifier(dataset_train.inputs, 10, init_range=init_range)  \n","  optimizer = SGDOptimizer(model, learning_rate)\n","  \n","  best_acc_val = float('-inf')\n","  best_W = None\n","  num_continuous_fails = 0\n","  loss_avg_before = None\n","  \n","  if verbose:\n","    print('Num parameters {:d}'.format(model.num_parameters()))\n","    print('Batch size {:d}, learning rate {:.5f}, regularization_weight {:.5f}'.format(batch_size, learning_rate, regularization_weight))\n","      \n","  for epoch in range(max_num_epochs):\n","    loss_total = 0.\n","    num_correct = 0    \n","\n","    for X, y in dataset_train.generate_batch(batch_size):  # Shuffled in each epoch\n","      loss_sum, scores = model.forward(X, y, regularization_weight)\n","      loss_total += loss_sum\n","      preds = np.argmax(scores, axis=1)[:, np.newaxis]\n","      num_correct += np.sum(preds == y)\n","\n","      optimizer.step()\n","      optimizer.zero_grad()\n","\n","    loss_avg = loss_total / dataset_train.num_examples()\n","    acc_train = num_correct / dataset_train.num_examples() * 100. \n","    acc_val = evaluate_accuracy(model, dataset_val)\n","  \n","    if acc_val > best_acc_val:\n","      num_continuous_fails = 0\n","      best_acc_val = acc_val\n","      best_W = copy.deepcopy(model.W)\n","    else:\n","      num_continuous_fails += 1\n","      if num_continuous_fails > tolerance:\n","        if verbose: \n","            print('Early stopping')\n","        break\n","    \n","    if loss_avg_before is not None:\n","      if loss_avg_before - loss_avg < loss_improvement:  # Training loss has not improved sufficiently, decay the learning rate\n","        optimizer.modify_lr(optimizer.lr / decay)\n","        if verbose and decay != 1.0:\n","          print('Decaying learning rate to {:.5f}'.format(optimizer.lr))\n","    loss_avg_before = loss_avg \n","\n","    if verbose:\n","      print('End of epoch {:3d}:\\t loss avg {:10.4f}\\t acc train {:10.2f}\\t acc val {:10.2f}'.format(\n","          epoch + 1, loss_avg, acc_train, acc_val)) \n","\n","  model.W = best_W\n","  if verbose:\n","    print('Best acc val: {:10.2f}'.format(best_acc_val))\n","\n","  return model, best_acc_val, loss_avg, acc_train"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GvqXqrvsHK2K"},"source":["model, acc_val, loss_avg, acc_train = train(dataset_train, dataset_val, learning_rate=5., batch_size=24, decay=2, verbose=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K14JhIvP3Q-y"},"source":["# Hyperparameter Tuning"]},{"cell_type":"markdown","metadata":{"id":"a105KYwK3S3S"},"source":["Experiments usually involve heavy hyperparameter tuning. This means you try a few different values of key hyperparameters to find one that works best (on validation data). For stochastic gradient descent, two important hyperparameters are the *batch size* and the *learning rate*. These two interact, so you have to search over their joint space. Typically you change them on a logarithmic scale (e.g., 0.0001, 0.001, 0.01, 0.1 for learning rates, 16, 32, 64, 128, 256 for batch sizes). \n","\n","However, to focus on the effect of regularization, we will fix the batch size and learning rate to be some reasonable values and only vary the regularization weight. "]},{"cell_type":"code","metadata":{"id":"vOtOa60mKC5o"},"source":["model_best = None\n","best_acc_val = float('-inf')\n","for batch_size in [16]:\n","  for learning_rate in [0.1]:\n","    for regularization_weight in [0, 0.0001, 0.001, 0.01, 1.0, 10.0]:\n","      model, acc_val, loss_avg, acc_train = train(dataset_train, dataset_val, learning_rate=learning_rate, batch_size=batch_size, regularization_weight=regularization_weight, max_num_epochs=60)\n","      print('Lambda {:10.4f}\\t loss {:10.4f}\\t acc train {:2.2f}\\t acc val {:2.2f}'.format(regularization_weight, loss_avg, acc_train, acc_val))\n","      if acc_val > best_acc_val:\n","        best_acc_val = acc_val\n","        model_best = copy.deepcopy(model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z_NKduQmaWHn"},"source":["## Question\n","\n","What is the impact of regularization on training loss/accuracy and validation accuracy? \n"]},{"cell_type":"markdown","metadata":{"id":"ozl4qSPTYDaA"},"source":["# Qualitative Analysis"]},{"cell_type":"markdown","metadata":{"id":"4_C-Ffmbamt0"},"source":["## Weight Visualization"]},{"cell_type":"markdown","metadata":{"id":"HdJ0yWngYJrh"},"source":["Since the model parameters are $w_l \\in \\mathbb{R}^d$ for $l = 1 \\ldots 10$ where the $(i+1)$th dimension corresponds to how much the $i$-th pixel value (shifted due to the bias dimension) implies the $l$-th label (i.e., digit $l-1$), we can directly visualize the parameters."]},{"cell_type":"code","metadata":{"id":"odbpjdPGY7P3"},"source":["def visualize_model(model):\n","  fig = plt.figure(figsize=(20,10))\n","  for label in range(10):\n","    plt.subplot(2, 5, label + 1)\n","    plt.imshow(model.W[1:, label].reshape(28, 28))\n","    plt.title('Label {:d}'.format(label))\n","    plt.colorbar()\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TMAqlssOgG2s"},"source":["visualize_model(model_best)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Xizpp7TagS5"},"source":["### Question\n","\n","What do you observe about the visualized weights for each class label?"]},{"cell_type":"markdown","metadata":{"id":"jckFbmida-L4"},"source":["## Confusion Matrix\n"]},{"cell_type":"markdown","metadata":{"id":"AT6Ly4e1boug"},"source":["Because we only have 10 labels we can easily visualize the prediction behavior of the model through a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix). "]},{"cell_type":"code","metadata":{"id":"IFwDenkXb4T-"},"source":["def build_confusion_matrix(model, dataset_val, batch_size_eval=64):\n","  confusion_matrix = [[0 for _ in range(10)] for _ in range(10)]\n","  for X, y in dataset_val.generate_batch(batch_size_eval):\n","    preds = model.predict(X)\n","    for i in range(X.shape[0]):\n","      confusion_matrix[y[i, 0]][preds[i][0]] += 1\n","\n","  for label1 in range(10):\n","    num_label1_total = sum(confusion_matrix[label1])\n","    for label2 in range(10):\n","      confusion_matrix[label1][label2] /= num_label1_total / 100\n","\n","  return confusion_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QKjVg_8ddJVi"},"source":["df = pandas.DataFrame(build_confusion_matrix(model_best, dataset_val), \n","                      index=[str(digit) for digit in range(10)], columns=[str(digit) for digit in range(10)])\n","plt.figure(figsize = (15,10))\n","seaborn.heatmap(df, annot=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t-oQPqZJeNWe"},"source":["### Question\n","\n","What do you observe in the confusion matrix? "]},{"cell_type":"markdown","metadata":{"id":"8VDCLoUEfgzC"},"source":["# Small Data Regime"]},{"cell_type":"markdown","metadata":{"id":"rcnzvx0eg88f"},"source":["To study the effect of the amount of supervision, we will train the model on a small subset of the training dataset with 30 examples total (3 examples per digit). "]},{"cell_type":"code","metadata":{"id":"pOvBfDxCfle2"},"source":["dataset_train_small = MNISTDataset('train_small')\n","\n","print(dataset_train_small.num_examples())\n","for digit in range(10):\n","  print('Digit {:1d} now has only {:1d} training examples'.format(digit, dataset_train_small.label_count[digit]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ib6ntHjRheq-"},"source":["Let's again do a model selection over the choice of $\\lambda$ (regularization strength), with batch size/learning rate fixed to some reasonable values. "]},{"cell_type":"code","metadata":{"id":"W9DdEGxzfq43"},"source":["model_best_small = None\n","best_acc_val_small = float('-inf')\n","for batch_size in [8]:\n","  for learning_rate in [0.1]:\n","    for regularization_weight in [0, 0.0001, 0.001, 0.01, 1.0, 10.0]:\n","      model, acc_val, loss_avg, acc_train = train(dataset_train_small, dataset_val, learning_rate=learning_rate, batch_size=batch_size, regularization_weight=regularization_weight, max_num_epochs=60)\n","      print('Lambda {:10.4f}\\t loss {:10.4f}\\t acc train {:2.2f}\\t acc val {:2.2f}'.format(regularization_weight, loss_avg, acc_train, acc_val))\n","      if acc_val > best_acc_val_small:\n","        best_acc_val_small = acc_val\n","        model_best_small = copy.deepcopy(model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9PIEIvsXZwFc"},"source":["visualize_model(model_best_small)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Q6KO0u-gWYe"},"source":["df = pandas.DataFrame(build_confusion_matrix(model_best_small, dataset_val), \n","                      index=[str(digit) for digit in range(10)], columns=[str(digit) for digit in range(10)])\n","plt.figure(figsize = (15,10))\n","seaborn.heatmap(df, annot=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zz1My2VEh4cn"},"source":["## Question\n","\n","What do you observe about the *weight visualization* and *confusion matrix* **compared to the large training data** setting?\n"]},{"cell_type":"markdown","metadata":{"id":"ieOiLsHPh-EN"},"source":["# Kaggle Submission"]},{"cell_type":"markdown","metadata":{"id":"kKHR5GLJqQvy"},"source":["To make the assignment more engaging we have a [Kaggle competition](https://www.kaggle.com/c/rutgers-cs461-hw2-fall-2021)! We will make test predictions with the best model we can train on the full training dataset (best in validation accuracy). Don't use more training data (in particular, don't retrain on train+val), but you can go back to the hyperparameter tuning and search other values of\n","\n","- Batch size\n","- Learning rate\n","- Max number of epochs\n","- Learning rate decay\n","\n","and others. It's fine to \"guess\" at this or that value, but it's usually best to be systematic about it and sweep all configurations because we don't know which hyperparameters interact with which (e.g., learning rate interacts strongly with batch size). If you feel ambitious you might even try [different gradient-based optimization methods](https://ruder.io/optimizing-gradient-descent/) like Adagrad or Adam. \n"]},{"cell_type":"code","metadata":{"id":"3upDGpr6ghDi"},"source":["def create_kaggle_submission(model, dataset_test, netid, batch_size_eval=64):\n","  preds = []\n","  for X, _ in dataset_test.generate_batch(batch_size_eval):  # Not shuffled\n","    preds.extend(model.predict(X)[:, 0].tolist())\n","\n","  with open('/content/drive/My Drive/cs461hw2_{:s}.csv'.format(netid), 'w') as f:\n","    writer = csv.writer(f)\n","    writer.writerow(['id', 'category'])  # Header\n","    for i, pred in enumerate(preds):\n","        writer.writerow([i, pred])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"etS2wBS5kXYy"},"source":["# Use your best model (assumed model_best) with your NetID here.\n","create_kaggle_submission(model_best, dataset_test, 'mynetid') "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TZOw4jGcsjN3"},"source":["This creates a file in your Google Drive `cs461hw2_mynetid.csv`. You can upload it to Kaggle and check how you're doing compared to others on the public leaderboard (50% of the test data). The final evaluation is shown on the private leaderboard (the other 50% of the test data), which is not revealed until after the competition is over."]},{"cell_type":"markdown","metadata":{"id":"m8jGqf6yFYZZ"},"source":["## Question\n","\n","Report the result of your **best model** \n","- Training accuracy\n","- Validation accuracy \n","- Test accuracy on public leaderboard (from Kaggle submission)"]}]}