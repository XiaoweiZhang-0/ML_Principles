{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"hw5.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"2AD9NyrbcNtv"},"source":["import csv\n","import math\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import random\n","\n","from scipy.special import logsumexp  # to deal with potential overflow issues\n","\n","\n","def set_seed(seed):  # For reproducibility, fix random seeds.\n","  random.seed(seed)\n","  np.random.seed(seed)\n","\n","set_seed(42)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sNebMufyeQK1"},"source":["# Fashion MNIST"]},{"cell_type":"markdown","metadata":{"id":"9Y8Hr-p9eSul"},"source":["Download the Fashion MNIST dataset from [here](https://drive.google.com/drive/folders/1BnU7wVriolasZAZ1bSDTyll1Tp61hP1c?usp=sharing). It consists of 16 x 16 grayscale images (downsized from 28 x 28 for efficiency), split into 50,000 training and 10,000 validation images. Each image is labeled as one of 10 clothing categories (e.g., dress, sandal, shirt). We will assume that we have the directory `data/FashionMNIST/` in our Google Drive account. Let's load the data and stare at it.  "]},{"cell_type":"code","metadata":{"id":"zC6hxke1eztw"},"source":["# Load the Drive helper and mount. You will have to authorize this operation. \n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L7E9MOeqe5ho"},"source":["datadir = '/content/drive/My Drive/data/FashionMNIST/'\n","label_names = ['tshirt/top', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']  # Hardcoded\n","\n","class FashionMNIST:\n","\n","  def __init__(self, split):\n","    assert split in ['Tr', 'Vl']  # We don't have a test set\n","    self.inputs = np.load('{:s}x{:s}.npy'.format(datadir, split))  # (N, 16^2)\n","    self.labels = np.load('{:s}y{:s}.npy'.format(datadir, split))  # (N, 10)\n","    self.num_examples, self.dim = self.inputs.shape\n","    self.num_labels = self.labels.shape[1]\n","\n","    # Partition data by labels\n","    self.partition = [self.inputs[np.where(np.argmax(self.labels, axis=1) == y)[0]] for y in range(self.num_labels)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U7PzwMQYsIu9"},"source":["data_train = FashionMNIST('Tr')\n","data_val = FashionMNIST('Vl')\n","\n","print(data_train.inputs.shape, data_val.inputs.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1QeCGtK9cNtv"},"source":["def show_image(image, ax=None):\n","  width = int(np.sqrt(image.shape[0]))\n","  image = image.reshape(width, width)  \n","  if ax == None:\n","    fig = plt.figure()\n","    ax = fig.add_subplot(1, 1, 1)\n","  imgplot = ax.imshow(image, cmap=mpl.cm.Greys)\n","  imgplot.set_interpolation('nearest')\n","  ax.xaxis.set_ticks_position('top')\n","  ax.yaxis.set_ticks_position('left')\n","  plt.axis('off')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IAbGlNBDsFRl"},"source":["for y in range(10):\n","  ax = plt.subplot(2, 5, y + 1)\n","  ax.clear()\n","  show_image(data_train.partition[y][0], ax)\n","  ax.set_title(label_names[y])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gHe9VNhOCQNd"},"source":["The images are a little blurry because we downsized them, but they will do. "]},{"cell_type":"markdown","metadata":{"id":"QOwxgHSqcNtx"},"source":["# Gaussian Mixture Model (GMM)"]},{"cell_type":"markdown","metadata":{"id":"MWtBS_kLCbEw"},"source":["A **Gaussian mixture model** (GMM) with $K$ components has learnable parameters $\\pi \\in \\mathbb{R}^K$ and $\\mu_k \\in \\mathbb{R}^K, \\Sigma_k \\in \\mathbb{R}^{K \\times K}$ for each $k = 1 \\ldots K$, where it is always assumed that \n","\n","- $\\pi$ is a (full-support) distribution over the $K$ components. So $\\pi_k > 0$ and $\\sum_{k=1}^K \\pi_k = 1$. \n","- $\\Sigma_k$ is a symmetric and positive definite (thus invertible) for each $k$. \n","\n","A GMM  defines a joint distribution over $k \\in \\{1 \\ldots K\\}$ and $x \\in \\mathbb{R}^d$ by \n","$$\n","p(k,x) = \\pi_k \\times \\mathcal{N}(\\mu_k, \\Sigma_k)(x)\n","$$\n","where $\\mathcal{N}(\\mu_k, \\Sigma_k)(x)$ is the probability of $x$ under the Gaussian distribution with mean $\\mu_k$ and covariance matrix $\\Sigma_k$. For practical reasons, we do the following:  \n","\n","- We optionally restrict ourselves to *diagonal* covariance matrices to make calculation simpler and more efficient.\n","- We work in *log space* for numerical stability, that is: $\\log p(k,x) = \\log \\pi_k + \\log \\mathcal{N}(\\mu_k, \\Sigma_k)(x)$. \n","- We make all variables multidimensional tensors so that we can use linear algebraic operations instead of for loops. "]},{"cell_type":"code","metadata":{"id":"Sgn46V-QvXJf"},"source":["class GMM:\n","\n","  def __init__(self, dim, num_components, diag=False):\n","    self.pi = np.full(num_components, 1. / num_components)  # (K,)\n","    self.mu = np.zeros((num_components, dim))  # (K, d)\n","    if diag:\n","      self.sigma = np.ones((num_components, dim))  # (K, d)\n","    else:   \n","      self.sigma = np.array([np.identity(dim) for _ in range(num_components)])   # (K, d, d)\n","    self.diag = diag\n","\n","  def compute_log_probs(self, inputs):  # (N, d)\n","    log_pi = np.log(self.pi)[:, np.newaxis]  # (K, 1)\n","    diffs = inputs[np.newaxis, :, :] - self.mu[:, np.newaxis, :]  # (K, N, d)\n","\n","    # TODO: implement, do not use for loops\n","    log_probs = None  # (K, N): log p(k, inputs[i])\n","\n","    return log_probs  \n","\n","  def compute_posteriors(self, inputs):  # (N, d)\n","    log_probs = self.compute_log_probs(inputs)  # (K, N): log p(k, x)\n","    marginal_log_probs = logsumexp(log_probs, axis=0)  # (N,): log p(x)\n","    marginal_log_likelihood = marginal_log_probs.mean()  # Scalar\n","    posteriors = np.exp(log_probs - marginal_log_probs[np.newaxis, :])  # (K, N): p(k|x)\n","    return posteriors, marginal_log_likelihood, marginal_log_probs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QlLeHKH3PLQW"},"source":["import unittest\n","\n","from scipy.stats import multivariate_normal \n","\n","class TestGMM(unittest.TestCase):\n","      \n","  def setUp(self):\n","    set_seed(42)\n","    self.dim = 100\n","    self.num_components = 7\n","    self.num_examples = 200\n","    self.power = 2 # Check for numerical stability\n","\n","    self.inputs = np.random.randn(self.num_examples, self.dim)\n","\n","  def test_model_diag(self): \n","    model = self.init_model(diag=True)\n","    log_probs_gold = self.get_log_probs_gold(model)\n","    log_probs = model.compute_log_probs(self.inputs)\n","    for k in range(self.num_components):\n","      for i in range(self.num_examples):\n","        self.assertAlmostEqual(log_probs[k, i], log_probs_gold[k, i])\n","\n","  def test_model_nondiag(self): \n","    model = self.init_model(diag=False)\n","    log_probs_gold = self.get_log_probs_gold(model)\n","    log_probs = model.compute_log_probs(self.inputs)\n","    for k in range(self.num_components):\n","      for i in range(self.num_examples):\n","        self.assertAlmostEqual(log_probs[k, i], log_probs_gold[k, i])\n","\n","  def get_log_probs_gold(self, model):\n","    log_probs_gold = []\n","    for k in range(self.num_components):\n","      dist = multivariate_normal(mean=model.mu[k], cov=(np.diag(model.sigma[k]) if model.diag else model.sigma[k]))\n","      log_probs_gold.append([np.log(model.pi[k]) + dist.logpdf(self.inputs[i]) for i in range(self.num_examples)])\n","    return np.array(log_probs_gold)\n","\n","  def init_model(self, diag=False):\n","    model = GMM(self.dim, self.num_components, diag=diag)\n","    pi_unnormalized = np.random.uniform(size=(self.num_components,)) ** self.power\n","    model.pi = pi_unnormalized / pi_unnormalized.sum()\n","    model.mu = np.random.randn(self.num_components, self.dim) ** self.power \n","    if diag:\n","      model.sigma = np.random.randn(self.num_components, self.dim) ** self.power\n","    else:\n","      model.sigma = np.array([np.diag(np.random.randn(self.dim)) ** self.power for _ in range(self.num_components)])\n","    return model\n","            \n","unittest.main(TestGMM(), argv=[''], verbosity=2, exit=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Tm42OewPsZ4"},"source":["# Expectation Maximization (EM)"]},{"cell_type":"markdown","metadata":{"id":"xm87kuPpPyNg"},"source":["The EM algorithm trains a GMM on *unlabeled* data by alternating the E step and the M step:\n","- E step: Compute posteriors $p(k|x_i)$ for every training input $i = 1 \\ldots N$ and label $k = 1\\ldots K$. This needs an initial set of parameter values.\n","- M step: Calculate the maximum-likelihood estimate of parameters under the posteriors.\n"]},{"cell_type":"code","metadata":{"id":"CImDKOOo0c9m"},"source":["class GMMTrainerEM:\n","\n","  def __init__(self, model, smoothing=0.1):\n","    self.model = model\n","    self.smoothing = smoothing\n","    self.diag_smoother =  smoothing * np.array([np.identity(model.mu.shape[1]) for _ in range(model.mu.shape[0])])\n","\n","  def train(self, inputs, num_iterations_max=40, verbose=False, init_method='naive'):\n","    self.init_centers(inputs, init_method=init_method)\n","    mll_previous = -np.inf \n","    posteriors, mll, _ = self.model.compute_posteriors(inputs)  # E step\n","    for iteration in range(num_iterations_max): \n","      self.update_parameters(inputs, posteriors)  # M step\n","      posteriors, mll, _ = self.model.compute_posteriors(inputs)  # E step\n","      if verbose:\n","        print('Iteration {:3d}:\\t marginal log-likelihood {:10.4f}'.format(iteration + 1,  mll))\n","      if np.isclose(mll, mll_previous):\n","        break\n","      mll_previous = mll\n","    return mll, iteration\n","\n","  def update_parameters(self, inputs, posteriors):\n","    expected_counts = posteriors.sum(axis=1) + self.smoothing # (K,)\n","    self.model.pi = expected_counts / expected_counts.sum()\n","\n","    weighted_sums = posteriors @ inputs  # (K, d)\n","    self.model.mu = weighted_sums / expected_counts[:, np.newaxis]\n","\n","    diffs = inputs[np.newaxis, :, :] - self.model.mu[:, np.newaxis, :]  # (K, N, d)\n","    diffs_weighted = posteriors[:, :, np.newaxis] * diffs  # (K, N, d)\n","    if self.model.diag:\n","      self.model.sigma = None  # TODO: Implement\n","    else:\n","      self.model.sigma = None  # TODO: Implement\n","    \n","  def init_centers(self, inputs, init_method='naive'):\n","    # Find K centers from the given input vectors (N, d) somehow.\n","    if init_method == 'naive':\n","\n","      # TODO: Implement\n","      # Tip: try selecting a random input as the first center, then iteratively selecting argmax_i sum_{l<k} ||x_l - x_i||\n","      self.model.mu = None \n","\n","    else: \n","      raise ValueError('Unknown init method: ' + init_method)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FArYXwBYRECG"},"source":["# Experiments with Diagonal GMMs"]},{"cell_type":"markdown","metadata":{"id":"R6cqa4eNRGkA"},"source":["We can use GMMs for classification, by training a GMM for each input partition with the same label then at test time predicting the label corresponding to the GMM with highest *marginal* likelihood."]},{"cell_type":"code","metadata":{"id":"DtdXLfOAcNt0"},"source":["def compute_accuracy(models, data):  # models[y]: GMM for label y\n","  log_probs_all = np.zeros((data.num_examples, data.num_labels))\n","  for y in range(data.num_labels):\n","    log_probs_all[:, y] = None  # TODO: Implement\n","  preds = np.argmax(log_probs_all, axis=1)\n","  acc = np.mean(preds == np.argmax(data.labels, axis=1)) * 100.\n","  return acc, preds"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YO6Y0GreRjz-"},"source":["One cool thing is that each mean $\\mu_k$ corresponding to component $k$ can be visualized. We will hypothesize that different components learn different representations of the same label."]},{"cell_type":"code","metadata":{"id":"fY8xEOASMsiM"},"source":["def show_means(models, y):\n","  num_components = len(models[y].pi)\n","  fig, axes = plt.subplots(1, num_components)\n","\n","  for k in range(num_components):\n","    if num_components == 1:\n","      ax = axes\n","    else:\n","      ax = axes[k]\n","    show_image(models[y].mu[k], ax)\n","    ax.axis(\"off\")\n","    if k == 0:\n","      ax.set_title(label_names[y] + \"/\" + str(k))\n","    else:\n","      ax.set_title(\"/\" + str(k))\n","  return fig, axes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k4Q11-RoR4qm"},"source":["We're ready to train diagonal GMMs with various $K$ values. The training is pretty sensitive to the smoothing parameter so be careful. "]},{"cell_type":"code","metadata":{"id":"UWNGVVXpEuoP"},"source":["set_seed(0)\n","smoothing = 0.1\n","\n","for num_components in [1, 3, 5, 7]:\n","  print('***Training a diagonal GMM with K={:d} components***'.format(num_components))\n","  models = []\n","  for y in range(data_train.num_labels):\n","    model = GMM(data_train.dim, num_components, diag=True)\n","    trainer = GMMTrainerEM(model, smoothing=smoothing)\n","    models.append(model)\n","    mll, iteration = trainer.train(data_train.partition[y], num_iterations_max=40, verbose=False)\n","    print('Label {:d}: {:2d} iterations, final MLL {:10.3f}'.format(y, iteration, mll))\n","  acc_train, _ = compute_accuracy(models, data_train)\n","  acc_val, _ = compute_accuracy(models, data_val)\n","  print('K={:d}: acc train {:3.2f}, acc val {:3.2f}'.format(num_components, acc_train, acc_val))\n","  for y in range(10):\n","    show_means(models, y)\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GAAFkFm6SUVw"},"source":["## <font color=red>Question</font>\n","For GMMs with diagonal covariance matrices, report\n","- $K=1$: train acc, val acc\n","- $K=3$: train acc, val acc\n","\n","## <font color=red>Answer</font>\n","write your answer here"]},{"cell_type":"markdown","metadata":{"id":"HQV9dxI8S-sU"},"source":["# Experiments with Full-Covariance GMMs"]},{"cell_type":"markdown","metadata":{"id":"BGDG_wegUwHq"},"source":["Similarly we will train full-covariance GMMs for various $K$ values. Again, the training is pretty sensitive to the smoothing parameter. In fact, the marginal log-likelihood may take positive values (invalid!!!) due to numerical instability, but we can still do classification with the model."]},{"cell_type":"code","metadata":{"id":"GoZiCsjdTHWj"},"source":["set_seed(0)\n","smoothing = 0.001\n","\n","for num_components in [1, 3, 5, 7]:\n","  print('***Training a full-covariance GMM with K={:d} components***'.format(num_components))\n","  models = []\n","  for y in range(data_train.num_labels):\n","    model = GMM(data_train.dim, num_components, diag=False)\n","    trainer = GMMTrainerEM(model, smoothing=smoothing)\n","    models.append(model)\n","    mll, iteration = trainer.train(data_train.partition[y], num_iterations_max=40, verbose=False)\n","    print('Label {:d}: {:2d} iterations, final MLL {:10.3f}'.format(y, iteration, mll))\n","  acc_train, _ = compute_accuracy(models, data_train)\n","  acc_val, _ = compute_accuracy(models, data_val)\n","  print('K={:d}: acc train {:3.2f}, acc val {:3.2f}'.format(num_components, acc_train, acc_val))\n","  for y in range(10):\n","    show_means(models, y)\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fM6DhwLXUIFL"},"source":["## <font color=red>Question</font>\n","For GMMs with full covariance matrices, report\n","- $K=1$: train acc, val acc\n","- $K=3$: train acc, val acc\n","\n","## <font color=red>Answer</font>\n","write your answer here\n"]},{"cell_type":"markdown","metadata":{"id":"U2eO8SEJVuIx"},"source":["---\n","\n","## <font color=red>Question</font>\n","Explore the model space with different settings: varying number of components across classes (required), covariance matrix restrictions (required), initialization of GMM, convergence criteria of training, etc\n","- Report the setting and val accuracy of your single **best model**.\n","- Describe (in detail) the **trend in accuracies** (train and val) across settings\n","- Describe (in detail) the **trend in visualized cluster** (displayed by the showMeans function) across settings\n","\n","\n","## <font color=red>Answer</font>\n","write your answer here"]}]}