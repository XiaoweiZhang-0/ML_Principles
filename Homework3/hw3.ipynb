{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"hw3.ipynb","provenance":[],"toc_visible":true},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"kb_HrAkqwYPf"},"source":["%matplotlib inline\n","\n","import csv\n","import matplotlib \n","import matplotlib.pyplot as plt\n","import numpy as np\n","import random\n","\n","def set_seed(seed):  # For reproducibility, fix random seeds.\n","  random.seed(seed)\n","  np.random.seed(seed)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uzvaa_IIycJ4"},"source":["The SVM learning problem is a convex optimization problem. It is presented either in its primal or dual form. The primal form is one of norm minimization subject to constraints, while the dual is a quadratic programming problem that is typically solved with an off-the-shelf QP solver. As a result, most popular machine learning libraries (e.g., LIBSVM underneath sklearn) solve the dual. In this assignment, you will solve the primal using subgradient descent, both the simple linear version and a version that supports nonlinear kernels. That is, we will solve the following optimization problem (we're not learning a separate bias weight $b$ without loss of generality):\n","$$\n","\tw^* = \\arg\\!\\min_{w \\in \\mathbb{R}^d}\\;\\; \\frac{\\lambda}{2} ||w||^2 + \\frac{1}{N} \\sum_{i=1}^N [\\, 1 -  y_i(w \\cdot x_i) \\,]_+\n","$$\n","where $(x_1, y_1) \\ldots (x_N, y_N) \\in \\mathbb{R}^d \\times \\{ \\pm 1 \\}$ is training data and $[\\cdot]_+ = \\max(0, \\cdot)$."]},{"cell_type":"markdown","metadata":{"id":"_--B4iwulBIp"},"source":["# Synthetic Data"]},{"cell_type":"markdown","metadata":{"id":"Uupkx5YgjkXF"},"source":["To facilitate development, let's start by writing a class that represents a synthetic binary classification dataset. "]},{"cell_type":"code","metadata":{"id":"R6ydbxLTQ24T"},"source":["class Data: \n","  \"\"\"Parent class for data objects\"\"\"\n","  \n","  def generate_batch(self, batch_size, shuffle=True):\n","    inds = list(range(self.num_examples))\n","    if shuffle:\n","      random.shuffle(inds)    \n","    for i in range(0, len(inds), batch_size):\n","        inds_batch = inds[i: i + batch_size]\n","        X = self.inputs[inds_batch, :]\n","        y = self.labels[inds_batch] \n","        yield X, y, inds_batch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OLk6yx9LyUGO"},"source":["class Data2D(Data):\n","  \n","  def __init__(self, num_examples, boundary='line', input_min=-0.5, input_max=0.5):\n","    super().__init__()\n","    set_seed(42)\n","    self.inputs = np.random.uniform(input_min, input_max, (num_examples, 2))\n","    if boundary == 'line':\n","      labels = self.inputs.sum(axis=1) > 0\n","    elif boundary == 'circle':\n","      labels = (self.inputs ** 2).sum(axis=1) > ((input_max * 0.7) ** 2)\n","    else:\n","      raise ValueError('Unknown boundary: ' + boundary)\n","\n","    self.labels = 2 * labels - 1  # Convert binary labels from 0/1 to +1/-1    \n","    self.num_examples = num_examples\n","    self.dim = 2\n","    self.input_min = input_min\n","    self.input_max = input_max\n","\n","  def plot(self):\n","    plt.scatter(self.inputs[:, 0], self.inputs[:, 1], c=self.labels, cmap=matplotlib.cm.Paired)\n","    plt.plot()  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3kGAZn2bwYPd"},"source":["# Dataset of random points on the plane with true labels from a linear decision boundary\n","data_linear = Data2D(500, boundary='line') \n","data_linear.plot()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q1r0pz7Dk1FF"},"source":["# Dataset of random points on the plane with true labels from a nonlinear decision boundary\n","data_nonlinear = Data2D(500, boundary='circle')   \n","data_nonlinear.plot()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gix-eLQ6lJ4d"},"source":["# Linear SVM"]},{"cell_type":"markdown","metadata":{"id":"lBLfSSDb3AM8"},"source":["Implement the linear SVM class below."]},{"cell_type":"code","metadata":{"id":"w-DMteRgKuZ7"},"source":["class LinearSVM:\n","\n","  def __init__(self, dim, init_randn=False):\n","    self.w = np.random.randn(dim) if init_randn else np.zeros(dim) \n","\n","  def forward(self, X, y=None, la=1.):\n","    \"\"\"\n","    Given input vectors X (N x d), make N predictions (scores & labels).\n","    If also given gold labels y, compute the loss/gradient.\n","    Use regularization strength la (\"lambda\").\n","    \"\"\" \n","    scores = X.dot(self.w[:, np.newaxis]).squeeze() \n","    preds = 2 * (scores > 0) - 1  \n","    loss = None\n","    grad = None\n","    if y is not None:\n","      margins = None  # TODO: Compute the margins here.\n","\n","      # TODO: Use the margins to calculate the loss and the gradient here.\n","      \n","    return {'preds': preds, 'scores': scores, 'loss': loss, 'grad': grad}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DFDnC5CF3Gc1"},"source":["To help you check for correctness, the following unit test checks the output of your model against the output of the reference code. "]},{"cell_type":"code","metadata":{"id":"F4YykOMQ0Wr6"},"source":["import unittest\n","\n","class TestLinearSVM(unittest.TestCase):\n","\n","  def setUp(self):\n","    set_seed(42)\n","    dim = 3\n","    num_examples = 42\n","    self.model = LinearSVM(dim, init_randn=True)  # Random init, instead of 0\n","    self.X = np.random.randn(num_examples, dim)\n","    self.y = 2 * np.random.randint(2, size=(num_examples,)) - 1\n","    self.places = 4\n","      \n","  def test_model(self): \n","    output = self.model.forward(self.X, self.y, 0.01)\n","    true_loss = -0.24501151815868508\n","    true_grad = [0.16616191, -0.08949146,  0.24762397]\n","    self.assertAlmostEqual(output['loss'], true_loss, places=self.places)\n","    for i in range(len(true_grad)):\n","      self.assertAlmostEqual(output['grad'][i], true_grad[i], places=self.places)\n","            \n","unittest.main(TestLinearSVM(), argv=[''], verbosity=2, exit=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q8oScHzQ3qYB"},"source":["## Training"]},{"cell_type":"markdown","metadata":{"id":"q7l6-y_h30pv"},"source":["Evaluation function to compute classification accuracy."]},{"cell_type":"code","metadata":{"id":"X3AR-tcOat5c"},"source":["def evaluate(model, data, batch_size_eval=16):\n","  num_correct = 0\n","  for (X, y, _) in data.generate_batch(batch_size_eval, shuffle=False):\n","    output = model.forward(X)\n","    num_correct += (y == output['preds']).sum()\n","  acc = num_correct / data.num_examples * 100.\n","  return acc"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4CWUn39g35jo"},"source":["To train the model, we will use stochastic gradient descent (SGD) with batch size 1. In particular, we will use a dynamic learning rate schedule that sets the learning rate for update $t \\geq 1$ as \n","$$\n","\\eta_t = \\frac{1}{\\lambda t}\n","$$\n","Recall that $\\lambda > 0$ is the regularization hyperparamaeter, we will assume this is strictly positive. This learning rate schedule has a formal justification in the Pegasos algorithm."]},{"cell_type":"code","metadata":{"id":"HItkswo6UFd_"},"source":["def train_linear(data, la, max_num_epochs=20, seed=42, verbose=False):\n","  set_seed(seed)\n","  model = LinearSVM(data.dim) \n","  acc = 0.\n","  step = 1\n","  for epoch in range(1, max_num_epochs + 1):\n","    loss_total = 0.\n","    for (x, y, _) in data.generate_batch(1): \n","      output = model.forward(x, y, la=la)\n","      lr = 1 / (la * step)\n","      model.w -= lr * output['grad']\n","      loss_total += output['loss']\n","      step += 1\n","    acc = evaluate(model, data)\n","    if verbose:\n","      print('Epoch {:d}: avg loss {:.3f}, train acc {:.2f}'.format(epoch, loss_total / data.num_examples, acc))    \n","\n","  return model, acc"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GJrsLH4N5Zvt"},"source":["To help visualize the decision boundary, we will visualize the contour of model predictions."]},{"cell_type":"code","metadata":{"id":"mlvmYGutfvGh"},"source":["def draw_contour(model, data2d, M=100):\n","  # First, scatter plot actual data points.\n","  plt.scatter(data2d.inputs[:, 0], data2d.inputs[:, 1], c=data2d.labels, cmap=matplotlib.cm.Paired)\n","\n","  # Next, compute model predictions for (M, M) \"grid\" coordinates on the plane. \n","  # To do this, we need 2 * M^2 input values: M^2 values for dim 1 and M^2 values for dim 2.\n","  # We can get such evenly spaced values by NumPy's linspace and meshgrid.\n","  ticks = np.linspace(data2d.input_min, data2d.input_max, M)\n","  x1, x2 = np.meshgrid(ticks, ticks, indexing=\"ij\")\n","  inputs = np.stack([x1, x2], axis=2).reshape(-1, 2)\n","  preds = model.forward(inputs)['preds'].reshape(x1.shape)  # {+1, -1}^{M x M}\n","\n","  # Draw the contour based on the grid predictions. The bigger M is, the smoother the contour.\n","  plt.contourf(x1, x2, preds, cmap=matplotlib.cm.Paired, alpha=0.8)\n","  plt.plot()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ht7UD15YY2RI"},"source":["model_linear, acc = train_linear(data_linear, 0.01)\n","print('train acc {:.2f}'.format(acc))\n","draw_contour(model_linear, data_linear)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"65o_JX0FzyCt"},"source":["Does the model actually find a max margin boundary? Let's try a small training dataset where it's more visually clear. "]},{"cell_type":"code","metadata":{"id":"c3CDBKZ3y_8m"},"source":["data_small = Data2D(4, boundary='line')\n","model_linear_small, _ = train_linear(data_small, 0.01)\n","draw_contour(model_linear_small, data_small)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_DuEiNUk-G3f"},"source":["It seems to. Just for fun, let's try fitting a linear SVM on the nonlinear data."]},{"cell_type":"code","metadata":{"id":"uLW_F_6J-K8p"},"source":["model_linear_bad, acc = train_linear(data_nonlinear, 0.01)\n","print('train acc {:.2f}'.format(acc))\n","draw_contour(model_linear_bad, data_nonlinear)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xrfaaA9s-0pw"},"source":["Not surprisingly, best line predictions are not much better than random guesses on the circle data."]},{"cell_type":"markdown","metadata":{"id":"w7LZaKjnwYPk"},"source":["# Kernel SVM"]},{"cell_type":"markdown","metadata":{"id":"l-vr7Mt5852D"},"source":["## Kernels"]},{"cell_type":"markdown","metadata":{"id":"-bBuPGUW0aZH"},"source":["Let's start by implementing a few well-known kernels."]},{"cell_type":"code","metadata":{"id":"X7wrOYWQwYPl","executionInfo":{"status":"ok","timestamp":1634473015734,"user_tz":240,"elapsed":4,"user":{"displayName":"Karl Stratos","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgK1wwDxSVXZabCKDQW9jq4_GPHYMTdZxSDSzwdPQ=s64","userId":"06056584612284185506"}}},"source":["def construct_kernel(kernel_type, dim=1, offset=0., gamma=0.1):\n","  \"\"\"\n","  Return a function that takes X (N, d) and Y (M, d) and outputs a (N, M) matrix \n","  filled with kernel outputs.\n","  \"\"\"\n","  if kernel_type == 'linear':\n","    def kernel(X, Y):\n","      return X.dot(Y.T)\n","\n","  elif kernel_type == 'poly':\n","    def kernel(X, Y):\n","      return (offset + X.dot(Y.T)) ** dim\n","\n","  elif kernel_type == 'gaussian':\n","    def kernel(X, Y):\n","      # Insert a new axis so that we can broadcast.\n","      X = X[:, np.newaxis, :]  # (N, 1, d)\n","\n","      exponents = None  # TODO: Implement.\n","      return np.exp(exponents)\n","\n","  else:\n","    raise ValueError('Unknown kernel: ' + kernel)\n","\n","  return kernel"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y-drOuDB87dW"},"source":["## Model"]},{"cell_type":"markdown","metadata":{"id":"Qt3Zx1uQ6DwX"},"source":["A kernel SVM maintains $K$ support vectors $(x'_1, y'_1) \\ldots (x'_K, y'_K)$ which is a subset of the training data. Training involves identifying the support vectors and learning their weights $\\alpha_1 \\ldots \\alpha_K \\geq 0$, which implies the parameter $w_{\\mathrm{kernelized}} = \\sum_{k=1}^K \\alpha_k y'_k \\phi(x'_k) \\in \\mathcal{F}$ where $\\phi: \\mathbb{R}^d \\rightarrow \\mathcal{F}$ is an implicit feature mapping under the chosen kernel. For any $x \\in \\mathbb{R}^d$, the model computes the score \n","$$\n","w_{\\mathrm{kernelized}} \\cdot \\phi(x) = \\sum_{k=1}^K \\alpha_k y'_k (\\phi(x'_k) \\cdot \\phi(x)) = \\sum_{k=1}^K \\alpha_k y'_k K(x'_k, x)\n","$$"]},{"cell_type":"code","metadata":{"id":"lQiAyK_OEAyF"},"source":["class KernelSVM:\n","\n","  def __init__(self, dim, kernel):\n","    self.kernel = kernel\n","    self.support_X = np.zeros((1, dim))  # (K, d) where K is the number of support vectors\n","    self.support_y = np.zeros(1)  # K\n","    self.support_al = np.zeros(1)  # K\n","\n","  def forward(self, X):\n","    kernel_output = self.kernel(self.support_X, X)  # (K, N)\n","    scores = (self.support_al * self.support_y).dot(kernel_output)  # N\n","    preds = 2 * (scores > 0) - 1  \n","    return {'preds': preds, 'scores': scores}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cs1wn7PZ89NG"},"source":["## Training"]},{"cell_type":"markdown","metadata":{"id":"ubNEQaRA9IEq"},"source":["We will train a kernel SVM with the kernelized [Pegasos](https://home.ttic.edu/~nati/Publications/PegasosMPB.pdf) algorithm (see Fig. 3). It cleverly kernelizes the primal SVM objective optimized with SGD (with learning rate $\\frac{1}{\\lambda t}$) by noting that the parameter vector at update $t+1$ must always have the form \n","$$\n","w^{(t+1)} = \\frac{1}{\\lambda t} \\sum_{i=1}^N \\textbf{count}(i) y_i \\phi(x_i)\n","$$\n","where $\\textbf{count}(i)$ is the number of times the margin constraint is violated on the $i$-th example so far. This implies that we never have to explicitly compute $w$; we can maintain examples with nonzero counts as support vectors and the counts as their weights ($\\alpha$). "]},{"cell_type":"code","metadata":{"id":"zxEHqDiHJX8O"},"source":["def pegasos_kernelized(data, kernel, la, max_num_epochs=20, seed=42, verbose=False):\n","  set_seed(seed)\n","  model = KernelSVM(data.dim, kernel)\n","  violation_counts = np.zeros(data.num_examples)\n","  acc = 0.\n","  step = 1\n","  for epoch in range(1, max_num_epochs + 1):\n","    loss_total = 0.\n","    for (x, y, x_index) in data.generate_batch(1):\n","      output = model.forward(x)\n","      lr = 1 / (la * step)\n","      margin = None  # TODO: Compute the margin (single example).\n","      if margin < 1:\n","        # TODO: Update the counts.\n","\n","        # TODO: Update the parameters (i.e., support vectors and their weights)\n","        model.support_X = None \n","        model.support_y = None \n","        model.support_al = None \n","\n","      step += 1\n","\n","    acc = evaluate(model, data)\n","    if verbose:\n","      print('Epoch {:d}: train acc {:.2f}'.format(epoch, acc))    \n","\n","  return model, acc"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gnKDGVkN_2Md"},"source":["Can it fit nonlinear data? "]},{"cell_type":"code","metadata":{"id":"Kn3voVAx_1TT"},"source":["model_nonlinear, acc = pegasos_kernelized(data_nonlinear, construct_kernel('gaussian', gamma=0.5), 1e-4)\n","print('train acc {:.2f}'.format(acc))\n","draw_contour(model_nonlinear, data_nonlinear)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9DnT1gTYACfk"},"source":["Apparently yes. It can also fit linear data, though it has more potential to overfit. "]},{"cell_type":"code","metadata":{"id":"mBt5jTKjALPN"},"source":["model_nonlinear_small, _ = pegasos_kernelized(data_small, construct_kernel('gaussian', gamma=0.5), 1e-4)\n","draw_contour(model_nonlinear_small, data_small)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AaU5tem6A-zA"},"source":["# Twitter Sentiment Analysis\n"]},{"cell_type":"markdown","metadata":{"id":"uobLyS5AFjnl"},"source":["## Data"]},{"cell_type":"markdown","metadata":{"id":"3dAxnuRnD7Hq"},"source":["Now that our SVM is working on the toy dataset, let's do a simple sentiment analysis on [tweets on US airline service quality](https://www.kaggle.com/crowdflower/twitter-airline-sentiment/version/2). (WARNING: expletives unfiltered.) Download the data from [here](https://drive.google.com/drive/folders/1o3eBoB5Urj6Yf2UfHgOoDpzkv-DBp5jE?usp=sharing). We will assume that we have the directory `data/airline_tweets/` in our Google Drive account containing the provided data. "]},{"cell_type":"code","metadata":{"id":"3uSsHMn9CgQB"},"source":["# Load the Drive helper and mount. You will have to authorize this operation. \n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qtAJ0VZIEdEC"},"source":["The data is in the CSV format. The columns most relevant to our task are **airline_sentiment** and **text**."]},{"cell_type":"code","metadata":{"id":"qfJW59XDwYPp"},"source":["import os\n","import pandas as pd\n","\n","datadir = '/content/drive/My Drive/data/airline_tweets/'\n","\n","dataframe_train = pd.read_csv(os.path.join(datadir, 'train.csv'))\n","dataframe_val = pd.read_csv(os.path.join(datadir, 'val.csv'))\n","dataframe_test = pd.read_csv(os.path.join(datadir, 'devtest.csv'))\n","\n","print(dataframe_train.airline_sentiment.value_counts())\n","dataframe_train.head(3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W-0VCFq2GH8Y"},"source":["Note that the data is unbalanced, with significantly more negatives than neutral + positives (perhaps not surprisingly?). Therefore we group neutral and positive into one category and the final ratio of non-negative vs negative is about 1:2. This is consistent across train, val, and devtest."]},{"cell_type":"markdown","metadata":{"id":"zkgEoRbPFuM2"},"source":["## Data Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"aGuRgScsFwNQ"},"source":["We need to represent the data as a ($N \\times d$) matrix, but what we have on our hands is unstructured text. The simplest solution to transform an airline review into a vector is [bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model). We maintain a global vocabulary of word patterns gathered from our corpus, with single words such as \"great\", \"horrible\", and optionally consecutive words ($n$-grams) like \"friendly service\", \"luggage lost\". Suppose we have already collected a total of 10000 such patterns, to transform a sentence into a 10000-dimensional vector, we simply scan it and look for the patterns that appear and set their correponding entries to 1 and leave the rest at 0. What we end up with is a sparse vector that can be fed into SVMs. For this exercise we use the basic text processing routines in nltk and sklearn."]},{"cell_type":"code","metadata":{"id":"6t-CKFm5wYPo"},"source":["import re, nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rPQQOzpUwYPo"},"source":["stop_words = set(stopwords.words('english'))  # Try printing out stop words. \n","wordnet_lemmatizer = WordNetLemmatizer()\n","\n","def tokenize_normalize(tweet):\n","  only_letters = re.sub('[^a-zA-Z]', ' ', tweet)\n","  tokens = nltk.word_tokenize(only_letters)[2:]\n","  lower_case = [l.lower() for l in tokens]\n","  filtered_result = list(filter(lambda l: l not in stop_words, lower_case))\n","  lemmas = [wordnet_lemmatizer.lemmatize(t) for t in filtered_result]\n","  return lemmas\n","\n","print(dataframe_train.text[1])\n","print(tokenize_normalize(dataframe_train.text[1]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AvSNs-C_MQqA"},"source":["We'll use [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) in sklearn to scan our corpus, build the vocab, and change text into vectors. You can try 2-grams (aka. bigrams), but that'll make the vocab much larger. "]},{"cell_type":"code","metadata":{"id":"-Bae6NdtwYPp"},"source":["vectorizer = CountVectorizer(tokenizer=tokenize_normalize, token_pattern=None, \n","                             strip_accents='unicode', ngram_range=(1, 1))  \n","# Extract the vocabulary\n","vectorizer.fit(pd.concat([dataframe_train, dataframe_val, dataframe_test]).text)  \n","\n","# Take a peek at the vocabulary. We have the terms and the counts\n","print(list(vectorizer.vocabulary_.items())[:10])\n","print('Vocabulary size: {:d}'.format(len(vectorizer.vocabulary_)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F2bS23NpICAN"},"source":["Note that the input dimension is fairly large (=vocabulary size). Coming up with a manageable vector representation is a major topic in natural language processing."]},{"cell_type":"code","metadata":{"id":"8_2nP7XASQxl"},"source":["class DataTwitter(Data):\n","\n","  def __init__(self, dataframe, vectorizer):\n","    self.inputs = vectorizer.transform(dataframe.text).toarray()\n","\n","    # Convert 'positive', 'neutral' to +1 and 'negative' to -1.\n","    sentiments = dataframe['airline_sentiment'].tolist()\n","    self.labels = np.array([-1 if sentiment == 'negative' else 1 for sentiment in sentiments])\n","    (self.num_examples, self.dim) = self.inputs.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2c5HDXCTwYPp"},"source":["data_twitter_train = DataTwitter(dataframe_train, vectorizer)\n","data_twitter_val = DataTwitter(dataframe_val, vectorizer)\n","data_twitter_test = DataTwitter(dataframe_test, vectorizer)\n","print('Train data shape: {:d} x {:d}'.format(data_twitter_train.num_examples, data_twitter_train.dim))\n","print('Val data shape: {:d} x {:d}'.format(data_twitter_val.num_examples, data_twitter_val.dim))\n","print('Test data shape: {:d} x {:d}'.format(data_twitter_test.num_examples, data_twitter_test.dim))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZCT4Pw5vwYPq"},"source":["## Experiments\n","Train a linear SVM on the twitter sentiment data. Play around with different parameter settings (e.g., number of epochs, regularization strength) and find the best setting on the validation set, then evaluate on the devtest set when you're finished tuning. \n","\n","1. What is your best *val* performance? Also report what hyperparameter values you used. \n","2. What is your final *test* performance, using your single best model? Do you see a large gap between the validation and test accuracies? \n","\n","Feel free to explore and discuss additional results. For instance,\n","\n","- (Optional) Try to come up with a better text feature representation. We threw out all the emojis. >_< what a waste\n","- (Optional) Experiment with kernel SVM. Given the high feature dimensionality of our primitive text processing, we do not recommend using kernel SVM here. It could take a long time to train. If you reduce the feature dimensionality, then it's a different story. Alternatively you could try to improve the efficiency of the Pegasos implementation: the current implementation is pretty dumb since it completely rewrites support vectors and weights at every iteration which is almost certainly not necessary. "]}]}